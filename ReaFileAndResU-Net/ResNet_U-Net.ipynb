{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本文说明\n",
    "**本文内容:**  本次作业是把原始的[U-Net](https://arxiv.org/abs/1505.04597) 网络中的Encode部分的代码改成[ResNet](https://arxiv.org/abs/1512.03385) 的BottleNeck部分\n",
    "  - **第一部分**:U-Net内容理解与网络实现\n",
    "  - **第二部分**:U-Net的ResNet版本实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net网络理解与总结\n",
    "首先这里说到U-Net一般都会提到FCN，作为以神经网路为基础的图像语义分割领域，FCN有着重要的历史地位。在此基础之上U-Net作者提出了一种对称的，以Encoder和Decoder为BackBone的网络框架，该网络不仅分割精度高，同时只需要各更少的数据集。但是除了结构上的不同之外，U-Net和FCN之间还有那些差别呢？本文做出如下说明：\n",
    "  #### 与FCN的对比理解\n",
    "  - U-Net的Encoede采用的是5和stage的经典架构，而FCN则是以VGG为主要框架的卷积操作\n",
    "  - FCN采用的是先进行100的padding，然后其余每层再进行padding为1的操作，U-Net的padding数为0，所以个人理解是 ```` 后面的copy and crop 需要剪裁，是因为卷积操作中没有使用Padding而导致Encoder和Decoder对应卷积层的FeatureMap图像尺寸不同````同时论文也提到“ The cropping is necessary due to the loss of border pixels in every convolution. ”\n",
    "  - 在skip connect部分U-Net采用的是把Encoder对应层的Feature Map进行叠加操作，而FCN则是对最后1x1卷积得到的结果进行0，2，4倍的上采样再与对应尺寸的FeatureMap的各个Map逐像素相加而得到结果\n",
    "  \n",
    "#### U-Net优点总结\n",
    "  - 使用较少的训练集得到较好的结果\n",
    "  - 网络提取的精度更高，具有较高的应用价值\n",
    "  - U-Net作为一种新型的网络框架具有重要的意义，同时后面会尝试该框架与一些分类效果分类效果的网络进行融合是否回去的更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### U-Net网络结构分析\n",
    "  **首先是U-Net的网络结构分析**\n",
    "   - U-Net的Encoder是由5个stage组成，也就是经过4次下采样图像变为小于原来1/16的图像\n",
    "   - U-Net的中间部分是使用了两个全卷积的网络之后与Decoder连接、\n",
    "   - Decoder部分对图像进行上采样操作，利用转置卷积或双线性插值都可以，但是据说双线性插值效果略好于转置卷积\n",
    "   - skip connect是先对图像教进行剪裁，再把对应的FeatureMap进行叠加，这里用到的函数是torch.cat()函数，此函数的好处是不会增加心得维度信息<br/>                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net网络实现    \n",
    "   **1、**由于U-Net的Encode是有5个stage组成，每个stage都是一样的，都是两次3x3的卷积操作然后进行**ReLU**和**BN**层操作\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,norm_layer=True):\n",
    "        super(UnetDownBlock, self).__init__()\n",
    "        block_list = []\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        block_list.append(self.conv1)\n",
    "        block_list.append(self.relu1)\n",
    "        if norm_layer:\n",
    "            block_list.append(self.bn1)\n",
    "        block_list.append(self.conv2)\n",
    "        block_list.append(self.relu2)\n",
    "        if norm_layer:\n",
    "            block_list.append(self.bn2)\n",
    "        self.block = nn.Sequential(*block_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2、** 定义下采样的block，此处下采样有两种方法可以选择，一种是双线性插值上采样，另一种是转置卷积实现上采样，但是目前后者一般不常用了。下采样之后图像同样进行两层3x3的卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_cahnnels, up_method, norm_layer):\n",
    "        super(UnetUpBlock, self).__init__()\n",
    "        if up_method == \"upconv\":\n",
    "            self.up_func = nn.ConvTranspose2d(in_channels, out_cahnnels, kernel_size=2, stride=2)\n",
    "        elif up_method == \"upsample\":\n",
    "            self.up_func = nn.Sequential(\n",
    "                nn.Upsample(mode=\"bilinear\", scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_cahnnels, kernel_size=1),)\n",
    "        self.conv_block = UnetDownBlock(in_channels, out_cahnnels, norm_layer)\n",
    "    \n",
    "    #定义copy_crop操作\n",
    "    def copy_crop(self, bridge, target_size):\n",
    "        _, _, h, w = bridge.size()\n",
    "        diff_y = (h - target_size[0])//2\n",
    "        diff_x = (w - target_size[1])//2  \n",
    "        return bridge[:, :, diff_y:(diff_y+target_size[0]), \n",
    "                      diff_x:(diff_x+target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up_func(x)\n",
    "        add_map = self.copy_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, add_map], 1)\n",
    "        out = self.conv_block(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3、** 定义Encoder部分，高部分主要是进行5个stage的卷积操作，同时利用Maxpooling对图像进行下采样操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetEncode(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        norm_layer=False):\n",
    "        super(UnetEncode, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        \n",
    "        #下采样的过程\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UnetDownBlock(prev_channels, 2 ** (wf + i),norm_layer)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        \n",
    "        \n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "        blocks.append(x)\n",
    "\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4、** 定义UNet的整个网络过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = 2 ** (wf + depth-1)\n",
    "        \n",
    "        self.encode = UnetEncode()\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UnetUpBlock(prev_channels, 2 ** (wf + i), up_mode, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        blocks = self.encode(x)\n",
    "        x = blocks[-1]\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 2])\n",
    "\n",
    "        return self.last(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,1, 572,572))\n",
    "unet = UNet()\n",
    "unet.eval()\n",
    "y_unet = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unet.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 改U-Net Encoder部分\n",
    "在之前的ResNet网络模型中，已经清楚的了解到了ResNet网络模型的整图架构，以及50层以上和50层以下的网路结构之间的细微差别。50层以上的网络模型主要运用的是BasicBlock结构（两个3x3的卷积后面加一个残差），而50层以上的网络主要运用的是BottleNeck的结构（1x1—3x3 — 1x1）同一个block中有维度上的变化，这也发挥了1x1卷积的优势。\n",
    "\n",
    "##### 定义一个CBR的Layer，因为后面有很多1x1和3x3的CBR层，方便后面使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CBR_Layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, padding=3, stride=2):\n",
    "        super(CBR_Layer, self).__init__()\n",
    "        block = []\n",
    "        block.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                              padding = padding, stride = stride))\n",
    "        block.append(nn.ReLU(inplace=True))\n",
    "        block.append(nn.BatchNorm2d(out_channels))\n",
    "        self.block = nn.Sequential(*block)\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1、这里首先对BasicBlock的部分进行定义** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplances, plances, norm_layer=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.block = UnetDownBlock(inplances, plancest, norm_layer=norm_layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        out += identity\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2、定义BottleNeck部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        assert out_chans%4==0\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        self.block1 = ResBlock(in_chans,int(out_chans/4),kernel_size=1,padding=0) #压缩\n",
    "        self.block2 = ResBlock(int(out_chans/4),int(out_chans/4),kernel_size=3,padding=1) #提取特征\n",
    "        self.block3 = ResBlock(int(out_chans/4),out_chans,kernel_size=1,padding=0) #恢复\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        identity = x\n",
    "        \n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        \n",
    "        out += identity\n",
    "        \n",
    "        return out\n",
    "\n",
    "class DownBottleNeck(nn.Module):\n",
    "    expansion = 4 #此处是由于Bottleneck中一个block中FeatureMap的维度变化比例是1:1:4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n",
    "        super(DownBottleNeck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.cbr1 = CBR_Layer(inplanes, planes, kernel_size=3, stride=1,padding=1)\n",
    "        self.cbr2 = CBR_Layer(planes, planes, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv2d(planes, planes*self.expansion, kernel_size=1,padding=0,stride=stride)\n",
    "        self.conv3 = nn.Conv2d(planes, planes*self.expansion,kernel_size=1, stride=stride)\n",
    "        self.bn3 = norm_layer(planes*self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.cbr1(x)\n",
    "        out = self.cbr2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        identity = self.conv1(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(in_channels, layer_list,name=\"vgg\"):\n",
    "    layers = []\n",
    "    if name==\"vgg\":\n",
    "        for v in layer_list:\n",
    "            layers += [CBR_Layer(in_channels, v)]\n",
    "            in_channels = v\n",
    "    \n",
    "    elif name==\"resnet\":\n",
    "        #需要down进行下采样\n",
    "        layers += [DownBottleNeck(in_channels, layer_list[0])]\n",
    "        in_channels = layer_list[0]\n",
    "        \n",
    "        for v in layer_list[1:]:\n",
    "            layers += [BottleNeck(in_channels, v)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "            \n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, in_channels, layer_list ,net_name):\n",
    "        super(Layer, self).__init__()\n",
    "        \n",
    "        self.layer = make_layers(in_channels, layer_list, name=net_name)\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3、定义Res_U-Net的Encoder部分**\n",
    "  此处需要说明一下：    \n",
    "    1）U-Net也是分为5个stage  \n",
    "    2）原始U-Net中一个stage中只有一个Block，但是这里改成ResNet之后可以是多个Block  \n",
    "    3）原始U-Net中每一个Block中都是3x3的卷积，但是这里改成ResNet之后会有1x1-3x3-1x3的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet101(nn.Module):\n",
    "    '''\n",
    "    ResNet101 model \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ResNet101,self).__init__()\n",
    "        #self, in_ch,out_ch, kernel_size=3, padding=1, stride=1):\n",
    "        self.conv1 = CBR_Layer(3,64)\n",
    "        #ceil_mode 设置为true，否则分辨率不对\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=2,ceil_mode=True)\n",
    "        \n",
    "        self.conv2_1 = DownBottleNeck(64,64)\n",
    "        self.conv2_2 = BottleNeck(256,256)\n",
    "        self.conv2_3 = BottleNeck(256,256)\n",
    "        \n",
    "        self.layer3 = Layer(256,[512]*2,'resnet')\n",
    "        self.layer4 = Layer(512,[1024]*23,'resnet')\n",
    "        self.layer5 = Layer(1024,[2048]*3,'resnet')\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        f1 = self.conv1(x)\n",
    "        f2 = self.conv2_3(self.conv2_2(self.conv2_1(self.pool1(f1))))\n",
    "        \n",
    "        f3 = self.layer3(f2)\n",
    "        f4 = self.layer4(f3)\n",
    "        f5 = self.layer5(f4)\n",
    "        return [f2,f3,f4,f5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=1,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        super(ResNetUNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = 2 ** (wf + depth)\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        self.encode = ResNet101()\n",
    "        \n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(2,depth)):\n",
    "            self.up_path.append(\n",
    "                UnetUpBlock(prev_channels, 2 ** (wf + i), up_mode, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = self.encode(x)\n",
    "        x = blocks[-1]\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 2])\n",
    "\n",
    "        return self.last(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,3, 256,256))\n",
    "unet = ResNetUNet()\n",
    "unet.eval()\n",
    "y_unet = unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
